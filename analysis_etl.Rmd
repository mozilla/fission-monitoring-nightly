---
title: "Fission Monitoring Nightly: Analysis ETL"
author: "Corey Dow-Hygelund, Mozilla Data Science"
date: "`r Sys.Date()`"
output:
  html_document:
    mathjax: null
    toc: true
    toc_collapsed: false
    toc_depth: 5
    number_sections: true
    theme: cosmo
params:
    args: !r list()

---

<style>
body {
line-height: 1.4em;
width: 100%;
}
.plotly {
text-align: center;
width: 75vw;
position: relative;
margin-left: calc((100% - 75vw)/2);
}
.zimg img {
text-align: center;
width: 75vw;
position: relative;
margin-left: calc((100% - 75vw)/2);
}
.r {
background-color: white;
border: 0;
}

pre code, pre, code {
white-space: pre !important;
overflow-x: scroll !important;
word-break: keep-all !important;
word-wrap: initial !important;
}
.caption {
font-size: 80%;
font-style: italic;
font-weight:bold;
}

caption {
font-size: 80%;
font-style: italic;
font-weight: bold;
}

h3, .h3 {
margin-top: 100px;
}


</style>


```{r sources}
source('params.R')
source('query.R')
source('stats.R')
```


```{r imports}
library(bigrquery)
library(data.table)
library(dplyr)
```


# Arguments
```{r args}
print('ETL configuration')
project_id <- Sys.getenv("BQ_BILLING_PROJECT_ID")
print(glue('BigQuery Billing project: {project_id}'))
tbl.main <- Sys.getenv('BQ_INPUT_MAIN_TABLE')
print(glue('Import - Main table: {tbl.main}'))
tbl.crashes <- Sys.getenv('BQ_INPUT_CRASH_TABLE')
print(glue('Import - Crash table: {tbl.crashes}'))
tbl.analyzed <- Sys.getenv('BQ_OUTPUT_TABLE')
print(glue('Import - Crash table: {tbl.analyzed}'))

is.debug <- case_when(
  Sys.getenv("DEBUG") == 'false' ~ FALSE,
  Sys.getenv("DEBUG") == 'true' ~ TRUE,
  TRUE ~ FALSE
)
print(glue('Debugging ETL pipeline: {is.debug}'))

# Minimum build ID to process. Apply this as a query filter to retrieve only new, unprocessed, builds. 
min_build_id <- case_when(
  Sys.getenv("MIN_BUILD_ID") == '' ~ bq_project_query(project_id, build_min_build_id_query(tbl.main, num_build_dates)) %>%
    bq_table_download() %>%
    pull(max_build_date) %>% 
    format('%Y%m%d'),
  TRUE ~ Sys.getenv("MIN_BUILD_ID") 
)
print(glue('Processing builds >= {min_build_id}'))

```

```{r bug_fixes}
options(scipen = 20) # bigrquery bug: https://github.com/r-dbi/bigrquery/issues/395 
```


# Histogram Aggregation

Perform the histogram aggregation server-side. This is most easy achieved processing a histogram at a time. 

**TODO** There are several methods necessary for handling histograms. This will require a map of probe_name -> hist_agg_method. Defaulting to `summarize.hist` for the moment. 


```{r hist_agg_var}
results.hist <- list()
hists.raw <- list()  #TODO: For initial EDA: remove after histograms well-understood
```

```{r hist_agg}
print('Processing histograms')
process_histograms <- function(probe) {
  tryCatch({
    print(probe)
    hist_query <-
      build_hist_query(probes.hist[[probe]], slug, tbl.main, min_build_id)
     hist <- bq_project_query(project_id, hist_query)
     hist.df <- bq_table_download(hist) %>%
       as.data.table()
    
     hist.summary <- summarize.hist(hist.df) %>%
       mutate(probe = probe) %>%
       rename(branch = what)

     hist.raw <- hist.df
   },
  error = function(err) {
     print(glue("ERROR processsing {probe}: {err}"))
     return(NULL)
   })
  return(list(raw = hist.df, summary = hist.summary)) 
}

for (probe in names(probes.hist)) {
  hist.res <- process_histograms(probe)
  results.hist[[probe]] <- hist.res$summary
  hists.raw[[probe]] <- hist.res$raw
  if (is.debug)
    break
}

# save(hists.raw, results.hist, file='hists_raw.2df')
```


# Scalar Aggregation

Pull the each's build per daily average of the scalars. 
```{r scalar_import}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, slug, tbl.main, min_build_id))
scalar.df <- bq_table_download(scalar) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```

```{r scalar_agg_var}
results.scalar <- list()
```

```{r scalar_agg, warning=FALSE}
if (is.debug)
  bs_replicates <- 20

process_scalar <- function(probe) {
  tryCatch({
    scalar.res <-
      scalar.df[get(probe) < quantile(get(probe), perc.high, na.rm = TRUE),
                summarize.scalar(.SD[, list(id, branch, x =
                                              get(probe))], "x", bs_replicates, stat = mean.narm),
                by = build_id][, probe := probe][order(build_id, what),] %>%
      rename(branch = what)
  },
  error = function(err) {
    # error handler picks up where error was generated
    print(glue("ERROR processsing {probe}: {err}"))
    return(NULL)
  })
  return(scalar.res)
}

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max))) {
  print(probe)
  results.scalar[[probe]] <- process_scalar(probe)
  if (is.debug)
    break
}
```

# Crash Aggregation

```{r crash_import}
crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

```{r}
# FIXME: remove filter
crashes.df <- crashes.df %>%
  filter(build_id != '20200923') %>%
  mutate(branch = case_when(
    branch == 'false' ~ 'disabled',
    TRUE ~ 'enabled'
  )) %>%
  as.data.table()
```

```{r crashes_agg_var}
results.crashes <- list()
```

Process the per usage hour probes:
```{r crashes_agg, warning=FALSE}
process_crash <- function(probe) {
  tryCatch({
    print(probe)
    crash_res <-
      crashes.df[get(probe_per_hour) < quantile(get(probe_per_hour), perc.high, na.rm = TRUE),
                 summarize.scalar(.SD[, list(id, branch, x =
                                               get(probe_per_hour))], "x", bs_replicates, stat = mean.narm),
                 by = build_id][, probe := probe_per_hour][order(build_id, what), ] %>%
      rename(branch = what)
  },
  error = function(err) {
    # error handler picks up where error was generated
    print(glue("ERROR processsing {probe}: {err}"))
    return(NULL)
  })
  return(crash_res)
}

for (probe in names(probes.crashes)) {
  if (probe == 'STARTUP_CRASHES')
    next #FIXME: Error in boot(d, function(x, i) { : no data in call to 'boot'
  probe_per_hour <- paste(probe, '_PER_HOUR', sep = '')
  results.crashes[[probe_per_hour]] <- process_crash(probe_per_hour)
  if (is.debug)
    break
}
```

# Export

Combine the individual probes into a single `data.frame`

```{r combine}
final.df <- rbindlist(results.hist) %>%
  rbind(., rbindlist(results.scalar)) %>%
  rbind(., rbindlist(results.crashes)) %>%
  mutate(date_computed = Sys.Date())

final.nrow <- nrow(final.df)
```

Remove records corresponding to the newest builds

```{r delete_processed_builds}
bq_project_query(project_id, build_delete_build_records_query(tbl.analyzed, min_build_id ))
```

Export finalized dataset to BigQuery for display in dashboard

```{r export}
print(glue('Exporting {final.nrow} records'))
tbl.analyzed.chunks <-
  strsplit(tbl.analyzed, '\\.')[[1]]

bq_table(project = tbl.analyzed.chunks[1],
         dataset = tbl.analyzed.chunks[2],
         table   = tbl.analyzed.chunks[3]) %>%
  bq_table_upload(
    values = final.df,
    create_disposition = "CREATE_IF_NEEDED",
    write_disposition = "WRITE_APPEND",
    # write_disposition = "WRITE_TRUNCATE",
    fields = as_bq_fields(final.df),
    billing = project_id
  )
```


# TODO

* For scalar query, perform dense_rank on client_id to get `id` field
* Deal with scaling on crashes_per_usage_hour: add a fudge factor? (e.g. crashes per 1000 hours)
* Scalar boostrapping should all be done with same replicates. Statistically more sound, and MUCH faster. 
* dynamically name control/treatment in stats.R

* Missing Analysis Fields
- Add total number of unique users per branch per build
- 
* should "MEMORY_TOTAL" = 'payload.processes.histograms.memory_total' and not content?
* Purpose of: sum(coalesce(VALUE_SUM(checkerboard_severity,500))) 
* Handling `device_resets`?


# FIXME
* `FX_NUMBER_OF_UNIQUE_SITE_ORIGINS_PER_LOADED_TABS` is a composition of many histograms. Only using `payload.histograms.fx_number_of_unique_site_origins_per_loaded_tabs_1` for POC. 
