---
title: "Fission Monitoring Nightly: Analysis ETL"
author: "Managed by Data Science,`r sprintf(' rendered at %s PST',Sys.time())`"
output:
  html_document:
    mathjax: null
    toc: true
    toc_collapsed: false
    toc_depth: 5
    number_sections: true
    theme: cosmo
params:
    args: !r list()

---

<style>
body {
line-height: 1.4em;
width: 100%;
}
.plotly {
text-align: center;
width: 75vw;
position: relative;
margin-left: calc((100% - 75vw)/2);
}
.zimg img {
text-align: center;
width: 75vw;
position: relative;
margin-left: calc((100% - 75vw)/2);
}
.r {
background-color: white;
border: 0;
}

pre code, pre, code {
white-space: pre !important;
overflow-x: scroll !important;
word-break: keep-all !important;
word-wrap: initial !important;
}
.caption {
font-size: 80%;
font-style: italic;
font-weight:bold;
}

caption {
font-size: 80%;
font-style: italic;
font-weight: bold;
}

h3, .h3 {
margin-top: 100px;
}


</style>


```{r sources}
source('params.R')
source('query.R')
source('stats.R')
source('process.R')
```


```{r imports}
library(bigrquery)
library(data.table)
library(dplyr)
```


# Arguments
```{r args, echo=FALSE}
print('ETL configuration')
project_id <- Sys.getenv("BQ_BILLING_PROJECT_ID")
print(glue('BigQuery Billing project: {project_id}'))
tbl.main <- Sys.getenv('BQ_INPUT_MAIN_TABLE')
print(glue('Import - Main table: {tbl.main}'))
tbl.crashes <- Sys.getenv('BQ_INPUT_CRASH_TABLE')
print(glue('Import - Crash table: {tbl.crashes}'))
tbl.analyzed <- Sys.getenv('BQ_OUTPUT_TABLE')
print(glue('Export - Analyzed table: {tbl.analyzed}'))

is.debug <- case_when(
  Sys.getenv("DEBUG") == 'false' ~ FALSE,
  Sys.getenv("DEBUG") == 'true' ~ TRUE,
  TRUE ~ FALSE
)
print(glue('Debugging ETL pipeline: {is.debug}'))

# Minimum build ID to process. Apply this as a query filter to retrieve only new, unprocessed, builds. 
if (Sys.getenv("MIN_BUILD_ID") == ''){
  min_build_id <- bq_project_query(project_id, build_min_build_id_query(tbl.main, num_build_dates)) %>%
    bq_table_download() %>%
    pull(max_build_date) %>% 
    format('%Y%m%d')
  if (as.integer(min_build_id) < exp_min_build_id) min_build_id <- exp_min_build_id
} else min_build_id <- Sys.getenv("MIN_BUILD_ID") 

print(glue('Processing builds >= {min_build_id}'))

print(glue('Utilizing {bs_replicates} bootstrap relicates'))
```

```{r bug_fixes, echo=FALSE}
options(scipen = 20) # bigrquery bug: https://github.com/r-dbi/bigrquery/issues/395 
```


# Histogram Aggregation

Perform the histogram aggregation server-side. This is most easy achieved processing a histogram at a time. 

```{r hist_agg_var}
results.hist <- results.hist.win <- results.hist.osx <- results.hist.linux <- list()
# hists.raw <- list()  
```

```{r hist_agg}
print('Processing histograms')

for (probe in names(probes.hist)) {
  results.hist[[probe]] <- process_histograms(probe)$summary
  results.hist.win[[probe]] <- process_histograms(probe, os='Windows')$summary
  results.hist.osx[[probe]] <- process_histograms(probe, os='Mac')$summary
  results.hist.linux[[probe]] <- process_histograms(probe, os='Linux')$summary
  if (is.debug && probe != 'CHECKERBOARDING_SEVERITY')
     break
}
```


```{r hist_agg_95th}
print('Processing histograms: 95th percentile')

for (probe in names(probes.hist.perc.95)) {
  if (is.debug) # TODO: This analysis takes WAY too long. Optimized bootstrapping. 
    break
  results.hist[[probe]] <- process_histograms_95th(probe)$summary
  # results.hist[[probe]] <- process_histograms_95th(probe, os='Windows')$summary
  # results.hist[[probe]] <- process_histograms_95th(probe, os='Mac')$summary
  # results.hist[[probe]] <- process_histograms_95th(probe, os='Linux')$summary
}
```


# Scalar Aggregation{.tabset}

1. Pull the each's build per daily average of the scalars. 
2. Calculate means and confidence intervals. 

## OS: ALL

```{r scalar_import}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, probes.hist.max, slug, tbl.main, min_build_id))
scalar.df <- bq_table_download(scalar) %>%
     mutate(branch = case_when(
        branch == 'fission-enabled' ~ 'enabled',
         TRUE ~ 'disabled'
       )) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```

```{r scalar_agg, warning=FALSE}
results.scalar <- list()

if (is.debug)
  bs_replicates <- 20

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max), names(probes.hist.max))) {
  print(probe)
  results.scalar[[probe]] <- process_scalar(scalar.df, probe, perc.high, bs_replicates)
}
```


## OS: Windows

```{r scalar_import_win}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, probes.hist.max, slug, tbl.main, min_build_id, os='Windows'))
scalar.df <- bq_table_download(scalar) %>%
     mutate(branch = case_when(
        branch == 'fission-enabled' ~ 'enabled',
         TRUE ~ 'disabled'
       )) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```



```{r scalar_agg_win, warning=FALSE}
results.scalar.win <- list()

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max), names(probes.hist.max))) {
  print(probe)
  results.scalar.win[[probe]] <- process_scalar(scalar.df, probe, perc.high, bs_replicates)
}
```



## OS: Mac

```{r scalar_import_osx}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, probes.hist.max, slug, tbl.main, min_build_id, os='Mac'))
scalar.df <- bq_table_download(scalar) %>%
     mutate(branch = case_when(
        branch == 'fission-enabled' ~ 'enabled',
         TRUE ~ 'disabled'
       )) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```



```{r scalar_agg_osx, warning=FALSE}
results.scalar.osx <- list()

if (is.debug)
  bs_replicates <- 20

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max), names(probes.hist.max))) {
  print(probe)
  results.scalar.osx[[probe]] <- process_scalar(scalar.df, probe, perc.high, bs_replicates)
}
```



## OS: Linux

```{r scalar_import_linux}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, probes.hist.max, slug, tbl.main, min_build_id, os='Linux'))
scalar.df <- bq_table_download(scalar) %>%
     mutate(branch = case_when(
        branch == 'fission-enabled' ~ 'enabled',
         TRUE ~ 'disabled'
       )) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```



```{r scalar_agg_linux, warning=FALSE}
results.scalar.linux <- list()

if (is.debug)
  bs_replicates <- 20

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max), names(probes.hist.max))) {
  print(probe)
  results.scalar.linux[[probe]] <- process_scalar(scalar.df, probe, perc.high, bs_replicates)
}
```



# Crash Aggregation{.tabset}

1. Pull the each's build per daily average of the crashes.
2. Process the per usage hour probes. 
3. Process the distinct client crashing. 

## OS: All
```{r crash_import}
crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

Process the per usage hour probes. 
```{r crashes_agg, warning=FALSE}
results.crashes <- list()

for (probe in names(probes.crashes)) {
  probe_per_hour <- paste(probe, '_PER_HOUR', sep = '')
  results.crashes[[probe_per_hour]] <- process_crash(crashes.df, probe_per_hour, perc.high, bs_replicates)
  if (is.debug)
    break
}
```

```{r crashes_count_agg, warning=FALSE}
client_count.stat <- function(x) length(which(!is.na(x) & x>0))
for (probe in names(probes.crashes)) {
  probe_client_count <- paste(probe, '_CLIENT_COUNT', sep = '')
  results.crashes[[probe_client_count]] <- process_crash(crashes.df,
                                                         probe, 
                                                         perc.high,
                                                         bs_replicates,
                                                         stat = client_count.stat,
                                                         name = probe_client_count)
  if (is.debug)
    break
}
```


## OS: Windows
```{r crash_import_win}
crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id, os='Windows_NT'))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

Process the per usage hour probes. 
```{r crashes_agg_win, warning=FALSE}
results.crashes.win <- list()

for (probe in names(probes.crashes)) {
  probe_per_hour <- paste(probe, '_PER_HOUR', sep = '')
  results.crashes.win[[probe_per_hour]] <- process_crash(crashes.df, probe_per_hour, perc.high, bs_replicates)
  if (is.debug)
    break
}
```

```{r crashes_count_agg_win, warning=FALSE}
client_count.stat <- function(x) length(which(!is.na(x) & x>0))
for (probe in names(probes.crashes)) {
  probe_client_count <- paste(probe, '_CLIENT_COUNT', sep = '')
  results.crashes.win[[probe_client_count]] <- process_crash(crashes.df,
                                                             probe, 
                                                             perc.high,
                                                             bs_replicates,
                                                             stat = client_count.stat,
                                                             name = probe_client_count)
  if (is.debug)
    break
}
```



## OS: Mac
```{r crash_import_mac}
crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id, os='Darwin'))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

Process the per usage hour probes. 
```{r crashes_agg_mac, warning=FALSE}
results.crashes.osx <- list()

for (probe in names(probes.crashes)) {
  probe_per_hour <- paste(probe, '_PER_HOUR', sep = '')
  results.crashes.osx[[probe_per_hour]] <- process_crash(crashes.df, probe_per_hour, perc.high, bs_replicates)
  if (is.debug)
    break
}
```

```{r crashes_count_agg_mac, warning=FALSE}
client_count.stat <- function(x) length(which(!is.na(x) & x>0))
for (probe in names(probes.crashes)) {
  probe_client_count <- paste(probe, '_CLIENT_COUNT', sep = '')
  results.crashes.osx[[probe_client_count]] <- process_crash(crashes.df,
                                                             probe, 
                                                             perc.high,
                                                             bs_replicates,
                                                             stat = client_count.stat,
                                                             name = probe_client_count)
  if (is.debug)
    break
}
```


## OS: Linux
```{r crash_import_linux}
crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id, os='Linux'))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

Process the per usage hour probes. 
```{r crashes_agg_linux, warning=FALSE}
results.crashes.linux <- list()

for (probe in names(probes.crashes)) {
  probe_per_hour <- paste(probe, '_PER_HOUR', sep = '')
  results.crashes.linux[[probe_per_hour]] <- process_crash(crashes.df, probe_per_hour, perc.high, bs_replicates)
  if (is.debug)
    break
}
```

```{r crashes_count_agg_linux, warning=FALSE}
client_count.stat <- function(x) length(which(!is.na(x) & x>0))
for (probe in names(probes.crashes)) {
  probe_client_count <- paste(probe, '_CLIENT_COUNT', sep = '')
  results.crashes.linux[[probe_client_count]] <- process_crash(crashes.df, 
                                                             probe, 
                                                         perc.high,
                                                         bs_replicates,
                                                         stat = client_count.stat,
                                                         name = probe_client_count)
  if (is.debug)
    break
}
```



# Export

Combine the individual probes into a single `data.frame`.

```{r combine}
final.df <- rbindlist(results.hist) %>%
  mutate(os = 'All') %>%
  rbind(., rbindlist(results.hist.win) %>% mutate(os = 'Windows')) %>%
  rbind(., rbindlist(results.hist.osx) %>% mutate(os = 'Mac')) %>%
  rbind(., rbindlist(results.hist.linux) %>% mutate(os = 'Linux')) %>%
  rbind(., rbindlist(results.scalar) %>% mutate(os = 'All')) %>%
  rbind(., rbindlist(results.scalar.win) %>% mutate(os = 'Windows')) %>%
  rbind(., rbindlist(results.scalar.osx) %>% mutate(os = 'Mac')) %>%
  rbind(., rbindlist(results.scalar.linux) %>% mutate(os = 'Linux')) %>%
  rbind(., rbindlist(results.crashes) %>% mutate(os = 'All')) %>%
  rbind(., rbindlist(results.crashes.win) %>% mutate(os = 'Windows')) %>%
  rbind(., rbindlist(results.crashes.osx) %>% mutate(os = 'Mac')) %>%
  rbind(., rbindlist(results.crashes.linux) %>% mutate(os = 'Linux')) %>%
  mutate(date_computed = Sys.Date())

final.nrow <- nrow(final.df)
```

Remove records corresponding to the newest builds

```{r delete_processed_builds}
bq_project_query(project_id, build_delete_build_records_query(tbl.analyzed, min_build_id ))
```

Export finalized dataset to BigQuery for display in dashboard

```{r export}
print(glue('Exporting {final.nrow} records'))
tbl.analyzed.chunks <-
  strsplit(tbl.analyzed, '\\.')[[1]]

bq_table(project = tbl.analyzed.chunks[1],
         dataset = tbl.analyzed.chunks[2],
         table   = tbl.analyzed.chunks[3]) %>%
  bq_table_upload(
    values = final.df,
    create_disposition = "CREATE_IF_NEEDED",
    write_disposition = "WRITE_APPEND",
    fields = as_bq_fields(final.df),
    billing = project_id
  )
```

# Cleanup 

Remove all objects from memory to not blow up anything (e.g. dashboard processing) downstream.

```{r gc}
rm(list=ls())
gc()
gc()
```

# TODO

* Deletion of records should be smart: e.g., only delete metric fields with successful analysis. 
* For scalar query, perform dense_rank on client_id to get `id` field
* Scalar bootstrapping should all be done with same replicates. Statistically more sound, and MUCH faster. 
* Dynamically name control/treatment in stats.R
* Adding `device_resets` for stability. 
