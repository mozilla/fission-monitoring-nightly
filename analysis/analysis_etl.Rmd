---
title: "Fission Monitoring Nightly: Analysis ETL"
author: "Managed by Data Science,`r sprintf(' rendered at %s PST',Sys.time())`"
output:
  html_document:
    mathjax: null
    toc: true
    toc_collapsed: false
    toc_depth: 5
    number_sections: true
    theme: cosmo
params:
    args: !r list()

---

<style>
body {
line-height: 1.4em;
width: 100%;
}
.plotly {
text-align: center;
width: 75vw;
position: relative;
margin-left: calc((100% - 75vw)/2);
}
.zimg img {
text-align: center;
width: 75vw;
position: relative;
margin-left: calc((100% - 75vw)/2);
}
.r {
background-color: white;
border: 0;
}

pre code, pre, code {
white-space: pre !important;
overflow-x: scroll !important;
word-break: keep-all !important;
word-wrap: initial !important;
}
.caption {
font-size: 80%;
font-style: italic;
font-weight:bold;
}

caption {
font-size: 80%;
font-style: italic;
font-weight: bold;
}

h3, .h3 {
margin-top: 100px;
}


</style>


```{r sources}
source('params.R')
source('query.R')
source('stats.R')
source('process.R')
```


```{r imports}
library(bigrquery)
library(data.table)
library(dplyr)
```


# Arguments
```{r args, echo=FALSE}
print('ETL configuration')
project_id <- Sys.getenv("BQ_BILLING_PROJECT_ID")
print(glue('BigQuery Billing project: {project_id}'))
tbl.main <- Sys.getenv('BQ_INPUT_MAIN_TABLE')
print(glue('Import - Main table: {tbl.main}'))
tbl.crashes <- Sys.getenv('BQ_INPUT_CRASH_TABLE')
print(glue('Import - Crash table: {tbl.crashes}'))
tbl.analyzed <- Sys.getenv('BQ_OUTPUT_TABLE')
print(glue('Export - Analyzed table: {tbl.analyzed}'))

is.debug <- case_when(
  Sys.getenv("DEBUG") == 'false' ~ FALSE,
  Sys.getenv("DEBUG") == 'true' ~ TRUE,
  TRUE ~ FALSE
)
print(glue('Debugging ETL pipeline: {is.debug}'))

# Minimum build ID to process. Apply this as a query filter to retrieve only new, unprocessed, builds. 
if (Sys.getenv("MIN_BUILD_ID") == ''){
  min_build_id <- bq_project_query(project_id, build_min_build_id_query(tbl.crashes, num_build_dates)) %>%
    bq_table_download() %>%
    pull(max_build_date) %>% 
    format('%Y%m%d')
  if (as.integer(min_build_id) < exp_min_build_id) min_build_id <- exp_min_build_id
} else min_build_id <- Sys.getenv("MIN_BUILD_ID") 

print(glue('Processing builds >= {min_build_id}'))

print(glue('Utilizing {bs_replicates} bootstrap relicates'))
```

```{r bug_fixes, echo=FALSE}
options(scipen = 20) # bigrquery bug: https://github.com/r-dbi/bigrquery/issues/395 
```

# Crash Table

Create crash table.

```{r crash_table_create}
export_tbl_chunks <- strsplit(tbl.crashes, '\\.')[[1]]

min_build_date <- bq_project_query(project_id, build_min_build_id_query(tbl.crashes, num_build_dates)) %>%
  bq_table_download() %>%
  pull(max_build_date) %>%
  format('%Y-%m-%d')

bq_project_query(project_id, 
                 build_crash_table(min_build_id = min_build_id, min_build_date = min_build_date), 
                 destination_table = bq_table(export_tbl_chunks[1], export_tbl_chunks[2], export_tbl_chunks[3]),
                 write_disposition = "WRITE_TRUNCATE"
                 )

```


# Histogram Aggregation

Perform the histogram aggregation server-side. This is most easy achieved processing a histogram at a time. 

```{r hist_agg_var}
results.hist <- results.hist.win <- results.hist.osx <- results.hist.linux <- list()
# hists.raw <- list()  
```

```{r hist_agg}
print('Processing histograms')

for (probe in names(probes.hist)) {
  results.hist[[probe]] <- process_histograms(probe)$summary
  results.hist.win[[probe]] <- process_histograms(probe, os='Windows')$summary
  results.hist.osx[[probe]] <- process_histograms(probe, os='Mac')$summary
  results.hist.linux[[probe]] <- process_histograms(probe, os='Linux')$summary
  if (is.debug && probe != 'CHECKERBOARDING_SEVERITY')
     break
}
```


```{r hist_agg_95th}
print('Processing histograms: 95th percentile')

for (probe in names(probes.hist.perc.95)) {
  if (is.debug) # TODO: This analysis takes WAY too long. Optimized bootstrapping. 
    break
  results.hist[[probe]] <- process_histograms_95th(probe)$summary
  # results.hist[[probe]] <- process_histograms_95th(probe, os='Windows')$summary
  # results.hist[[probe]] <- process_histograms_95th(probe, os='Mac')$summary
  # results.hist[[probe]] <- process_histograms_95th(probe, os='Linux')$summary
}
```


# Scalar Aggregation{.tabset}

1. Pull the each's build per daily average of the scalars. 
2. Calculate means and confidence intervals. 

## OS: ALL

```{r scalar_import}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, probes.hist.max, slug, tbl.main, 
                                                          min_build_id, min_build_date))
scalar.df <- bq_table_download(scalar) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```

```{r scalar_agg, warning=FALSE}
results.scalar <- list()

if (is.debug)
  bs_replicates <- 20

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max), names(probes.hist.max))) {
  print(probe)
  results.scalar[[probe]] <- process_scalar(scalar.df, probe, perc.high, bs_replicates)
}
```


## OS: Windows

```{r scalar_import_win}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, probes.hist.max, slug, tbl.main, 
                                                          min_build_id, min_build_date, os='Windows'))
scalar.df <- bq_table_download(scalar) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```



```{r scalar_agg_win, warning=FALSE}
results.scalar.win <- list()

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max), names(probes.hist.max))) {
  print(probe)
  results.scalar.win[[probe]] <- process_scalar(scalar.df, probe, perc.high, bs_replicates)
}
```



## OS: Mac

```{r scalar_import_osx}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, probes.hist.max, slug, tbl.main, 
                                                          min_build_id, min_build_date, os='Mac'))
scalar.df <- bq_table_download(scalar) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```



```{r scalar_agg_osx, warning=FALSE}
results.scalar.osx <- list()

if (is.debug)
  bs_replicates <- 20

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max), names(probes.hist.max))) {
  print(probe)
  results.scalar.osx[[probe]] <- process_scalar(scalar.df, probe, perc.high, bs_replicates)
}
```



## OS: Linux

```{r scalar_import_linux}
scalar <- bq_project_query(project_id, build_scalar_query(probes.scalar.sum, probes.scalar.max, probes.hist.max, slug, tbl.main, 
                                                          min_build_id, min_build_date, os='Linux'))
scalar.df <- bq_table_download(scalar) %>%
  as.data.table()

scalar.df.nrow <- nrow(scalar.df)
if (is.debug) print(glue('Processing scalar data.frame {scalar.df.nrow}'))
```



```{r scalar_agg_linux, warning=FALSE}
results.scalar.linux <- list()

if (is.debug)
  bs_replicates <- 20

for (probe in c(names(probes.scalar.sum), names(probes.scalar.max), names(probes.hist.max))) {
  print(probe)
  results.scalar.linux[[probe]] <- process_scalar(scalar.df, probe, perc.high, bs_replicates)
}
```



# Crash Aggregation{.tabset}

1. Pull the each's build per daily average of the crashes.
2. Process the per usage hour probes. 
3. Process the distinct client crashing. 

## OS: All
```{r crash_import}

crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.ui <-  bq_project_query(project_id, 
                                build_crash_ui_query(probes.crashes.ui, slug, 
                                                  tbl.main, min_build_id)) %>%
  bq_table_download() %>%
  as.data.table()
  
crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

Process the per usage hour probes. 
```{r crashes_agg, warning=FALSE}
results.crashes <- calc_crash_stats(crashes.df, crashes.ui)
```

## OS: Windows
```{r crash_import_win}
crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id, os='Windows_NT'))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.ui <-  bq_project_query(project_id, 
                                build_crash_ui_query(probes.crashes.ui, slug, 
                                                  tbl.main, min_build_id, os='Windows')) %>%
  bq_table_download() %>%
  as.data.table()

crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

Process the per usage hour probes. 
```{r crashes_agg_win, warning=FALSE}
results.crashes.win <- calc_crash_stats(crashes.df, crashes.ui)
```


## OS: Mac
```{r crash_import_mac}
crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id, os='Darwin'))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.ui <-  bq_project_query(project_id, 
                                build_crash_ui_query(probes.crashes.ui, slug, 
                                                  tbl.main, min_build_id, os='Mac')) %>%
  bq_table_download() %>%
  as.data.table()

crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

Process the per usage hour probes. 
```{r crashes_agg_mac, warning=FALSE}
results.crashes.osx <- calc_crash_stats(crashes.df, crashes.ui)
```


## OS: Linux
```{r crash_import_linux}
crashes <-  bq_project_query(project_id, build_crash_query(probes.crashes, slug, tbl.crashes, min_build_id, os='Linux'))
crashes.df <- bq_table_download(crashes) %>%
  as.data.table()

crashes.ui <-  bq_project_query(project_id, 
                                build_crash_ui_query(probes.crashes.ui, slug, 
                                                  tbl.main, min_build_id, os='Linux')) %>%
  bq_table_download() %>%
  as.data.table()


crashes.df.nrow <- nrow(crashes.df)
if (is.debug) print(glue('Processing crash data.frame {crashes.df.nrow}'))
```

Process the per usage hour probes. 
```{r crashes_agg_linux, warning=FALSE}
results.crashes.linux <- calc_crash_stats(crashes.df, crashes.ui)
```

# Export

Combine the individual probes into a single `data.frame`.

```{r combine}
final.df <- rbindlist(results.hist) %>%
  mutate(os = 'All') %>%
  rbind(., rbindlist(results.hist.win) %>% mutate(os = 'Windows')) %>%
  rbind(., rbindlist(results.hist.osx) %>% mutate(os = 'Mac')) %>%
  rbind(., rbindlist(results.hist.linux) %>% mutate(os = 'Linux')) %>%
  rbind(., rbindlist(results.scalar) %>% mutate(os = 'All')) %>%
  rbind(., rbindlist(results.scalar.win) %>% mutate(os = 'Windows')) %>%
  rbind(., rbindlist(results.scalar.osx) %>% mutate(os = 'Mac')) %>%
  rbind(., rbindlist(results.scalar.linux) %>% mutate(os = 'Linux')) %>%
  rbind(., rbindlist(results.crashes) %>% mutate(os = 'All')) %>%
  rbind(., rbindlist(results.crashes.win) %>% mutate(os = 'Windows')) %>%
  rbind(., rbindlist(results.crashes.osx) %>% mutate(os = 'Mac')) %>%
  rbind(., rbindlist(results.crashes.linux) %>% mutate(os = 'Linux')) %>%
  mutate(date_computed = Sys.Date())

final.nrow <- nrow(final.df)
```

Remove records corresponding to the newest builds

```{r delete_processed_builds}
bq_project_query(project_id, build_delete_build_records_query(tbl.analyzed, min_build_id ))
```

Export finalized dataset to BigQuery for display in dashboard

```{r export}
print(glue('Exporting {final.nrow} records'))
tbl.analyzed.chunks <-
  strsplit(tbl.analyzed, '\\.')[[1]]

bq_table(project = tbl.analyzed.chunks[1],
         dataset = tbl.analyzed.chunks[2],
         table   = tbl.analyzed.chunks[3]) %>%
  bq_table_upload(
    values = final.df,
    create_disposition = "CREATE_IF_NEEDED",
    write_disposition = "WRITE_APPEND",
    fields = as_bq_fields(final.df),
    billing = project_id
  )
```

# Cleanup 

Remove all objects from memory to not blow up anything (e.g. dashboard processing) downstream.

```{r gc}
rm(list=ls())
gc()
gc()
```

# TODO

* Deletion of records should be smart: e.g., only delete metric fields with successful analysis. 
* For scalar query, perform dense_rank on client_id to get `id` field
* Scalar bootstrapping should all be done with same replicates. Statistically more sound, and MUCH faster. 
* Dynamically name control/treatment in stats.R
* Adding `device_resets` for stability. 
